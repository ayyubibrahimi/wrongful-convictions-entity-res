{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ayyub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "\n",
    "def create_blocking_keys(df, first_name_col, last_name_col):\n",
    "    \"\"\"\n",
    "    Create blocking keys for the dataframe based on the first three letters of the first and last names.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing the relevant columns.\n",
    "    first_name_col (str): Column name for the first name.\n",
    "    last_name_col (str): Column name for the last name.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with an additional 'blocking_key' column.\n",
    "    \"\"\"\n",
    "    # Extract the first three characters of first and last names for blocking keys\n",
    "    # If names are shorter than three characters, use the entire name\n",
    "    df['first_name_key'] = df[first_name_col].str.lower().str[:3]\n",
    "    df['last_name_key'] = df[last_name_col].str.lower().str[:3]\n",
    "\n",
    "    # Combine the keys to form a composite blocking key\n",
    "    df['blocking_key'] = df['first_name_key'] + '_' + df['last_name_key']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_pairs(df, blocking_key_col):\n",
    "    \"\"\"\n",
    "    Generate record pairs within each block defined by the blocking key.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame with a 'blocking_key' column.\n",
    "    blocking_key_col (str): The name of the column containing the blocking keys.\n",
    "\n",
    "    Returns:\n",
    "    List of tuples: Each tuple contains the indices of two records to be compared.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    # Group the dataframe by the blocking key and iterate over the groups\n",
    "    for _, group in df.groupby(blocking_key_col):\n",
    "        # Generate all combinations of record pairs within each block\n",
    "        block_pairs = list(combinations(group.index, 2))\n",
    "        pairs.extend(block_pairs)\n",
    "    return pairs\n",
    "\n",
    "def create_comparison_dataframe(df, pairs_to_compare):\n",
    "    \"\"\"\n",
    "    Create a new dataframe where each row represents a pair of records,\n",
    "    with columns formatted like the training data.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Original DataFrame containing the records.\n",
    "    pairs_to_compare (List of tuples): Each tuple contains the indices of two records to be compared.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with each row representing a pair of records.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    for index1, index2 in pairs_to_compare:\n",
    "        record1 = df.loc[index1]\n",
    "        record2 = df.loc[index2]\n",
    "\n",
    "        pair_dict = {\n",
    "            'entity_1_first_name': record1['first_name'],\n",
    "            'entity_1_last_name': record1['last_name'],\n",
    "            'entity_1_rank': record1['officer_role'],  #\n",
    "            'entity_1_context': record1['officer_context'],\n",
    "            'entity_2_first_name': record2['first_name'],\n",
    "            'entity_2_last_name': record2['last_name'],\n",
    "            'entity_2_rank': record2['officer_role'],\n",
    "            'entity_2_context': record2['officer_context']\n",
    "        }\n",
    "        comparison_data.append(pair_dict)\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def calculate_context_similarity(contexts1, contexts2, tokenizer, model):\n",
    "    # Tokenize and encode both sets of contexts\n",
    "    encoded_input_1 = tokenizer(contexts1, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input_2 = tokenizer(contexts2, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Forward pass through BERT model for both sets of contexts\n",
    "    with torch.no_grad():\n",
    "        output_1 = model(**encoded_input_1)\n",
    "        output_2 = model(**encoded_input_2)\n",
    "    \n",
    "    # Extract embeddings and average across token dimension for each context\n",
    "    embeddings_1 = output_1.last_hidden_state.mean(dim=1)\n",
    "    embeddings_2 = output_2.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Convert embeddings to numpy arrays for cosine similarity calculation\n",
    "    embeddings_1_np = embeddings_1.detach().cpu().numpy()\n",
    "    embeddings_2_np = embeddings_2.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate cosine similarity between each pair of context embeddings\n",
    "    similarities = [cosine_similarity(embeddings_1_np[i].reshape(1, -1), embeddings_2_np[i].reshape(1, -1))[0][0] for i in range(len(contexts1))]\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def tokenize_and_stem(context):\n",
    "    tokens = word_tokenize(context)  \n",
    "    stemmed_tokens = stem_tokens(tokens)  \n",
    "    return stemmed_tokens\n",
    "\n",
    "def calculate_stemmed_shared_tokens(context1, context2):\n",
    "    tokens1 = set(tokenize_and_stem(context1))\n",
    "    tokens2 = set(tokenize_and_stem(context2))\n",
    "    return len(tokens1.intersection(tokens2))\n",
    "\n",
    "def calculate_stemmed_jaccard_similarity(context1, context2):\n",
    "    tokens1 = set(tokenize_and_stem(context1))\n",
    "    tokens2 = set(tokenize_and_stem(context2))\n",
    "    if not tokens1 or not tokens2: \n",
    "        return 1.0\n",
    "    return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "\n",
    "def string_similarity(s1, s2):\n",
    "    return difflib.SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "\n",
    "def feature_engineering(df, first_name_col_1, last_name_col_1, rank_col_1, context_col_1,\n",
    "                        first_name_col_2, last_name_col_2, rank_col_2, context_col_2):\n",
    "    \n",
    "    df = df.fillna(\"\")\n",
    "    \n",
    "    df[first_name_col_1] = df[first_name_col_1].fillna(\"\")\n",
    "    df[first_name_col_2] = df[first_name_col_2].fillna(\"\")\n",
    "    \n",
    "    df['first_name_similarity'] = df.apply(lambda x: string_similarity(x[first_name_col_1], x[first_name_col_2]), axis=1)\n",
    "    df['last_name_similarity'] = df.apply(lambda x: string_similarity(x[last_name_col_1], x[last_name_col_2]), axis=1)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    context_similarities = calculate_context_similarity(df[context_col_1].tolist(), df[context_col_2].tolist(), tokenizer, model)\n",
    "    df['context_similarity'] = context_similarities\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_blocking_keys(df):\n",
    "    df.fillna('', inplace=True)\n",
    "    \n",
    "    df['last_name_blocking_key'] = df['entity_1_last_name'].str.lower().str[:3] + '_' + df['entity_2_last_name'].str.lower().str[:3]\n",
    "    df['first_name_blocking_key'] = df['entity_1_first_name'].str.lower().str[:3] + '_' + df['entity_2_first_name'].str.lower().str[:3]\n",
    "    \n",
    "    df['context_similar'] = np.where(df['context_similarity'].astype(float) > 0.7, 'context_similar', 'context_not_similar')\n",
    "    \n",
    "    df['combined_blocking_key'] = df.apply(lambda x: '_'.join([\n",
    "        x['first_name_blocking_key'],\n",
    "        x['last_name_blocking_key'],\n",
    "        x['context_similar'],\n",
    "    ]), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def yield_record_pairs(df):\n",
    "    grouped = df.groupby('combined_blocking_key')\n",
    "    for _, group in grouped:\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        for pair in combinations(group.index, 2):\n",
    "            yield pair\n",
    "\n",
    "def prepare_features_for_model(df, record_pairs):\n",
    "    features = []\n",
    "    for pair in record_pairs:\n",
    "        record1, record2 = df.loc[pair[0]], df.loc[pair[1]]\n",
    "        feature_vector = [\n",
    "            float(record1['first_name_similarity']),\n",
    "            float(record1['last_name_similarity']),\n",
    "            float(record1['context_similarity']),\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "    return np.array(features)  \n",
    "\n",
    "def generate_candidate_pairs(df, trained_model):\n",
    "    record_pairs = list(yield_record_pairs(df))\n",
    "    X_test = prepare_features_for_model(df, record_pairs)  \n",
    "\n",
    "    predictions = trained_model.predict(X_test)\n",
    "\n",
    "    likely_matches = [pair for pair, prediction in zip(record_pairs, predictions) if prediction == 1]\n",
    "    candidate_pairs = pd.DataFrame({\n",
    "        'RecordID_1': [pair[0] for pair in likely_matches],\n",
    "        'RecordID_2': [pair[1] for pair in likely_matches],\n",
    "        'predicted_match': [1]*len(likely_matches)\n",
    "    })\n",
    "    \n",
    "    return candidate_pairs\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../ts-cluster/data/output/output-clean.csv\")\n",
    "\n",
    "df_with_blocking_keys = create_blocking_keys(df, 'first_name', 'last_name')\n",
    "\n",
    "pairs_to_compare = generate_pairs(df_with_blocking_keys, 'blocking_key')\n",
    "\n",
    "comparison_df = create_comparison_dataframe(df_with_blocking_keys, pairs_to_compare)\n",
    "\n",
    "comparison_df = comparison_df.drop_duplicates()\n",
    "comparison_df = comparison_df.iloc[:500]\n",
    "comparison_df\n",
    "\n",
    "comparison_df = feature_engineering(comparison_df, 'entity_1_first_name', 'entity_1_last_name', 'entity_1_rank', 'entity_1_context',\n",
    "                         'entity_2_first_name', 'entity_2_last_name', 'entity_2_rank', 'entity_2_context')\n",
    "\n",
    "comparison_df = generate_blocking_keys(comparison_df)\n",
    "\n",
    "\n",
    "with open('../../ts-train-model/data/output/trained_blocking_model.pkl', 'rb') as f:\n",
    "    trained_model = pickle.load(f)\n",
    "\n",
    "candidate_pairs = generate_candidate_pairs(comparison_df, trained_model)\n",
    "candidate_pairs.predicted_match.unique()\n",
    "\n",
    "# initial_candidates_count = len(pairs_to_compare)\n",
    "# print(f\"Initial number of candidate pairs for review: {initial_candidates_count}\")\n",
    "\n",
    "# candidates_after_model_count = len(candidate_pairs)\n",
    "# print(f\"Number of candidate pairs for review after model prediction: {candidates_after_model_count}\")\n",
    "\n",
    "# reduction_percentage = ((initial_candidates_count - candidates_after_model_count) / initial_candidates_count) * 100\n",
    "# print(f\"Reduction in candidate pairs due to model filtering: {reduction_percentage:.2f}%\")\n",
    "\n",
    "# candidate_pairs.to_csv('../data/output/candidate_pairs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
